import { NextRequest } from 'next/server'
import { streamText, convertToCoreMessages } from 'ai'
import { z } from 'zod'
import type { Message } from 'ai'
import { AI_MODELS, getDefaultModel, AIModelType } from '@/lib/ai/models'
import { MasterAgentV2 } from '@/lib/agents/master-agent-v2'
import { createClient } from '@/lib/supabase/server'
import { buildEnhancedContext, convertContextToMessages, getContextStats } from '@/lib/ai/context-builder'

// è¯·æ±‚éªŒè¯ - æ”¯æŒä¸¤ç§æ ¼å¼ï¼šä¼ ç»Ÿå•æ¶ˆæ¯å’Œæ–°çš„æ¶ˆæ¯æ•°ç»„
const requestSchema = z.object({
  // æ–°æ ¼å¼ï¼šæ¶ˆæ¯æ•°ç»„ï¼ˆç¬¦åˆ Vercel AI SDK æ ‡å‡†ï¼‰
  messages: z.array(z.object({
    id: z.string().optional(),
    role: z.enum(['user', 'assistant', 'system']),
    content: z.string().optional().default(''),
    parts: z.array(z.object({
      type: z.string().optional(),
      text: z.string().optional(),
    })).optional(),
  })).optional(),
  
  // ä¼ ç»Ÿæ ¼å¼ï¼šå•æ¶ˆæ¯ï¼ˆå‘åå…¼å®¹ï¼‰
  message: z.string().optional(),
  
  // é€šç”¨å‚æ•°
  contextId: z.string().optional(),
  conversationId: z.string().optional(), // ä¿æŒå‘åå…¼å®¹
  model: z.string().optional(),
  aiModel: z.enum(['openai', 'anthropic', 'gemini']).optional(), // å‘åå…¼å®¹
  temperature: z.number().min(0).max(2).optional(),
  maxTokens: z.number().min(1).max(8192).optional(),
}).refine(data => data.messages || data.message, {
  message: "Either 'messages' or 'message' is required"
})

export async function POST(req: NextRequest) {
  try {
    // 1. è®¤è¯æ£€æŸ¥
    const supabase = await createClient()
    const { data: { user }, error } = await supabase.auth.getUser()
    
    if (error || !user) {
      console.error('Auth check error:', error?.message || 'No user found')
      return new Response(
        JSON.stringify({ error: 'Unauthorized - Please login first' }), 
        { status: 401, headers: { 'Content-Type': 'application/json' } }
      )
    }

    // 2. éªŒè¯å’Œè§£æè¯·æ±‚
    const body = await req.json()
    const { 
      messages, 
      message, 
      contextId, 
      model, 
      aiModel, 
      temperature = 0.7, 
      maxTokens = 1024 
    } = requestSchema.parse(body)

    // 3. ç»Ÿä¸€æ¶ˆæ¯æ ¼å¼å¤„ç†
    let processedMessages: Message[]
    let userMessage: string

    if (messages) {
      // æ–°æ ¼å¼ï¼šä½¿ç”¨æ¶ˆæ¯æ•°ç»„ï¼Œå¤„ç†contentæˆ–partsæ ¼å¼
      processedMessages = messages
        .map(msg => {
          // æå–å†…å®¹ï¼šä¼˜å…ˆä½¿ç”¨contentï¼Œå¦åˆ™ä»partsä¸­æå–
          let content = msg.content || ''
          if (!content && msg.parts && msg.parts.length > 0) {
            content = msg.parts
              .filter(part => part.type === 'text' && part.text)
              .map(part => part.text)
              .join('')
          }
          return {
            ...msg,
            id: msg.id || `msg-${Date.now()}`,
            content,
          }
        })
        .filter(msg => msg.content && msg.content.trim()) // è¿‡æ»¤ç©ºæ¶ˆæ¯
      
      userMessage = processedMessages
        .filter(m => m.role === 'user')
        .pop()?.content || ''
    } else {
      // ä¼ ç»Ÿæ ¼å¼ï¼šè½¬æ¢å•æ¶ˆæ¯ä¸ºæ•°ç»„
      processedMessages = [
        { id: 'user-msg', role: 'user', content: message! }
      ]
      userMessage = message!
    }
    
    // æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„ç”¨æˆ·æ¶ˆæ¯
    if (!userMessage || !userMessage.trim()) {
      return new Response(
        JSON.stringify({ error: 'No valid user message found' }), 
        { status: 400, headers: { 'Content-Type': 'application/json' } }
      )
    }
    
    console.log(`ğŸ§  AI Chat (SDK) processing: "${userMessage}" for context: ${contextId}`)

    // 4. æ„å»ºå¢å¼ºä¸Šä¸‹æ–‡ï¼ˆMCP + Slacké›†æˆï¼‰
    console.log(`ğŸ” Building enhanced context for message: "${userMessage}"`)
    const contextData = await buildEnhancedContext(userMessage, contextId)
    const contextStats = getContextStats(contextData)

    // 5. Master Agent å¤„ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
    let agentResult: any = null
    if (contextId) {
      try {
        const masterAgent = new MasterAgentV2(true)
        agentResult = await masterAgent.processUserRequest(userMessage, contextId)
      } catch (error) {
        console.warn('âš ï¸ Master Agent processing failed:', error)
        // ç»§ç»­å¤„ç†ï¼Œä½†ä¸ä½¿ç”¨ Master Agent ç»“æœ
      }
    }

    // 6. æ„å»ºå¢å¼ºçš„æ¶ˆæ¯åˆ—è¡¨
    let enhancedMessages: Message[]

    if (contextStats.hasContext) {
      // ä½¿ç”¨ MCP ä¸Šä¸‹æ–‡æ„å»ºæ¶ˆæ¯
      enhancedMessages = convertContextToMessages(contextData, userMessage)
      console.log(`âœ… Using enhanced context: ${contextStats.slackMessagesCount} Slack messages, ${contextStats.emailsCount} emails, ${contextStats.calendarEventsCount} events`)
    } else if (agentResult?.success && agentResult?.metadata?.strategy !== 'direct_response') {
      // é™çº§åˆ° Master Agent ä¸Šä¸‹æ–‡
      enhancedMessages = [
        {
          id: 'system-agent-context',
          role: 'system',
          content: `ä½ æ˜¯AI Brainæ™ºèƒ½åŠ©æ‰‹ã€‚ä»¥ä¸‹æ˜¯Master Agentæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š

ğŸ¯ **ç”¨æˆ·æ„å›¾**: ${agentResult?.metadata?.intent?.category || 'æœªçŸ¥'}
ğŸ“Š **ç½®ä¿¡åº¦**: ${agentResult?.metadata?.confidence || 0}
ğŸ”§ **å­ä»£ç†**: ${agentResult?.metadata?.subAgentsUsed?.join(', ') || 'æ— '}

è¯·åŸºäºè¿™äº›ä¿¡æ¯ï¼Œç”¨ä¸“ä¸šã€å‹å¥½çš„æ–¹å¼å›å¤ç”¨æˆ·çš„é—®é¢˜ã€‚`,
        },
        ...processedMessages
      ]
    } else {
      // åŸºæœ¬ç³»ç»Ÿæç¤º
      enhancedMessages = [
        {
          id: 'system-base',
          role: 'system',
          content: `ä½ æ˜¯AI Brainï¼Œä¸€ä¸ªæ™ºèƒ½çš„ä¼ä¸šå·¥ä½œåŠ©æ‰‹ã€‚ä½ å¯ä»¥ï¼š

1. **æ•°æ®æºç®¡ç†**: æŸ¥è¯¢å’Œåˆ†æSlackã€Jiraã€GitHubã€Google Workspaceç­‰å·¥å…·çš„æ•°æ®
2. **ä»»åŠ¡ç®¡ç†**: åˆ›å»ºã€åˆ†é…å’Œè·Ÿè¸ªä»»åŠ¡
3. **å›¢é˜Ÿåä½œ**: å®‰æ’ä¼šè®®ã€ç”ŸæˆæŠ¥å‘Šã€åˆ†æå·¥ä½œè¿›å±•
4. **æ™ºèƒ½æ´å¯Ÿ**: æä¾›åŸºäºæ•°æ®çš„å»ºè®®å’Œé¢„æµ‹

è¯·ç”¨ç®€æ´ã€ä¸“ä¸šä¸”æœ‰ç”¨çš„æ–¹å¼å›åº”ç”¨æˆ·ã€‚`,
        },
        ...processedMessages
      ]
    }

    // 6. æ¨¡å‹é€‰æ‹©ï¼ˆä¼˜å…ˆ SDK æ ¼å¼ï¼Œå‘åå…¼å®¹ä¼ ç»Ÿæ ¼å¼ï¼‰
    let selectedModel: AIModelType = getDefaultModel()
    
    if (model && model in AI_MODELS) {
      selectedModel = model as AIModelType
    } else if (aiModel) {
      // å‘åå…¼å®¹ï¼šæ˜ å°„ä¼ ç»Ÿæ¨¡å‹åç§°
      const modelMapping: Record<string, AIModelType> = {
        'openai': 'gpt-3.5-turbo',
        'anthropic': 'claude-3-haiku',
        'gemini': 'gemini-flash'
      }
      selectedModel = modelMapping[aiModel] || getDefaultModel()
    }

    const aiModelInstance = AI_MODELS[selectedModel]

    if (!aiModelInstance) {
      throw new Error(`Model ${selectedModel} not available`)
    }

    // 7. é«˜ç½®ä¿¡åº¦ç›´æ¥å“åº”ä¼˜åŒ– - ä½¿ç”¨æµå¼å“åº”ä¿æŒä¸ Vercel AI SDK å…¼å®¹
    if (agentResult?.success && 
        agentResult.metadata.strategy === 'direct_response' && 
        agentResult.metadata.confidence > 0.8) {
      
      // ä½¿ç”¨ streamText æ¥è¿”å›ç›´æ¥å“åº”ï¼Œä¿æŒæµå¼æ ¼å¼å…¼å®¹æ€§
      const result = await streamText({
        model: aiModelInstance,
        messages: [
          {
            role: 'system',
            content: 'ä½ æ˜¯AI Brainæ™ºèƒ½åŠ©æ‰‹ã€‚ç›´æ¥è¿”å›å‡†å¤‡å¥½çš„å“åº”ï¼Œæ— éœ€é¢å¤–å¤„ç†ã€‚'
          },
          {
            role: 'user',
            content: 'è¯·è¿”å›ä»¥ä¸‹å›å¤ï¼š' + agentResult.response
          }
        ],
        temperature: 0,
        maxTokens: 2000,
      })

      return result.toTextStreamResponse()
    }

    // 8. AI SDK æµå¼ç”Ÿæˆ
    // å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿ enhancedMessages å­˜åœ¨
    if (!enhancedMessages || !Array.isArray(enhancedMessages)) {
      enhancedMessages = [
        {
          id: 'system-fallback',
          role: 'system',
          content: 'ä½ æ˜¯AI Brainæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·ç”¨ç®€æ´ã€ä¸“ä¸šä¸”æœ‰ç”¨çš„æ–¹å¼å›åº”ç”¨æˆ·ã€‚'
        },
        ...processedMessages
      ]
    }


    const result = await streamText({
      model: aiModelInstance,
      messages: enhancedMessages.map(msg => ({
        role: msg.role as 'user' | 'assistant' | 'system',
        content: msg.content
      })),
      temperature,
      maxTokens,
      system: agentResult?.success ? 
        `ä¸Šä¸‹æ–‡å¢å¼ºä¿¡æ¯ï¼š
- ç”¨æˆ·æ„å›¾ç±»å‹ï¼š${agentResult?.metadata?.intent?.category || 'æœªçŸ¥'}
- ä½¿ç”¨çš„å­ä»£ç†ï¼š${agentResult?.metadata?.subAgentsUsed?.join(', ') || 'æ— '}
- å¤„ç†ç­–ç•¥ï¼š${agentResult?.metadata?.strategy || 'æœªçŸ¥'}
- å¤„ç†æ—¶é—´ï¼š${agentResult?.metadata?.processingTime || 0}ms

è¯·å……åˆ†åˆ©ç”¨è¿™äº›ä¿¡æ¯æä¾›ç²¾ç¡®ã€æœ‰ç”¨çš„å›å¤ã€‚` : undefined,
    })

    // 9. è¿”å›æ–‡æœ¬æµå“åº”
    return result.toTextStreamResponse()

  } catch (error) {
    if (error instanceof z.ZodError) {
      return new Response(
        JSON.stringify({ error: 'Invalid request format', details: error.errors }), 
        { status: 400, headers: { 'Content-Type': 'application/json' } }
      )
    }
    
    console.error('AI Chat (SDK) Error:', error)
    
    // é™çº§åˆ°ä¼ ç»Ÿå“åº”æ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
    return new Response(
      JSON.stringify({ 
        response: 'AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚',
        error: 'AI service temporarily unavailable',
        model: 'fallback',
        timestamp: new Date().toISOString(),
        details: error instanceof Error ? error.message : 'Unknown error'
      }), 
      { 
        status: 500,
        headers: { 'Content-Type': 'application/json' }
      }
    )
  }
}

// GET è¯·æ±‚ï¼šè·å–å¯ç”¨æ¨¡å‹ï¼ˆæ–°å¢åŠŸèƒ½ï¼‰
export async function GET() {
  try {
    const { getAvailableModels, MODEL_METADATA } = await import('@/lib/ai/models')
    
    const available = getAvailableModels()
    const availableModels = Object.entries(available)
      .filter(([_, isAvailable]) => isAvailable)
      .map(([model]) => ({
        id: model,
        ...MODEL_METADATA[model as AIModelType],
      }))

    return new Response(JSON.stringify({
      success: true,
      models: availableModels,
      default: getDefaultModel(),
      capabilities: {
        streaming: true,
        masterAgent: true,
        contextIntegration: true,
        mcpSupport: true,
      }
    }), {
      headers: { 'Content-Type': 'application/json' }
    })
  } catch (error) {
    return new Response(
      JSON.stringify({ error: 'Failed to get available models' }),
      { status: 500, headers: { 'Content-Type': 'application/json' } }
    )
  }
}